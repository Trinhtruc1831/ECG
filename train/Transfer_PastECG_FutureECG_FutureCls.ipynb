{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/MIT-BIH/'\n",
    "minute_input = 10\n",
    "minute_output = 10\n",
    "window_input= 40*minute_input\n",
    "window_out= 40*minute_input\n",
    "train_size = 0.8\n",
    "test_size = 1 - train_size\n",
    "data_set = {\n",
    "  0: \"test\",\n",
    "  1: \"train\"\n",
    "}\n",
    "model_name_reg_phase = \"LSTM\"\n",
    "model_name_cls_phase = \"LSTM\"\n",
    "# length_ecg l√† ƒë·ªô d√†i 2 kho·∫£ng RR ƒë∆∞·ª£c fixed l√∫c ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu \n",
    "# (ƒë·ªô d√†i m·ªôt d√≤ng trong file excel, tr·ª´ c·ªôt cu·ªëi l√† nh√£n l·ªõp b·ªánh tim)\n",
    "length_ecg = 187 \n",
    "\n",
    "'''\n",
    "ƒê·ªô d√†i c·ªßa input/output c√†ng d√†i th√¨ s·ªë l∆∞·ª£ng file kh√¥ng ƒë√°p ·ª©ng ƒë·ªß ƒë·ªÉ t·∫°o m·ªôt m·∫´u h·ª£p l·ªá \n",
    "cho m√¥ h√¨nh c√†ng nhi·ªÅu. ƒê·ªÉ ƒë√°m b·∫£o t√≠nh th·ªëng nh·∫•t n√™n s·∫Ω d√πng ƒë·ªô d√†i d√†i nh·∫•t c·ªßa ph·∫ßn \n",
    "input/output trong qu√° tr√¨nh th·ª±c nghi·ªám ƒë·ªÉ l√† chu·∫©n t·ª´ ƒë√≥ lo·∫°i c√°c file b·ªã thi·∫øu n√†y ƒë·ªÅu\n",
    "·ªü nh·ªØng ph·∫ßn th·ª±c nghi·ªám input/output kh√°c.\n",
    "'''\n",
    "missing_file_train = ['201_V1.csv', '102_V2.csv', '124_V4.csv', '112_V1.csv', '203_V1.csv', '116_V1.csv', '108_V1.csv', '207_V1.csv', '111_V1.csv', '200_V1.csv', '207_MLII.csv', '210_V1.csv', '202_V1.csv', '113_V1.csv', '214_V1.csv', '121_V1.csv', '109_V1.csv', '105_V1.csv', '107_V1.csv', '115_V1.csv', '208_V1.csv']\n",
    "missing_file_test = ['213_V1.csv', '231_V1.csv', '228_V1.csv', '222_V1.csv', '232_V1.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # read data\n",
    "        X = self.data[i]\n",
    "        y = np.concatenate(self.label[i])\n",
    "        return X, y\n",
    "\n",
    "class Dataloader(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataset, batch_size,size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.size= size\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # collect batch data\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "\n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
    "        return tuple(batch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size //self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(istrainset):    \n",
    "    missing_file = []\n",
    "    total_sample = 0\n",
    "    directory = f\"{data_path}{data_set[istrainset]}/\"\n",
    "    X, y = [], []\n",
    "    for filename in os.listdir(directory):\n",
    "        if (filename not in (missing_file_train)) and (filename not in (missing_file_test)) and (filename != \".DS_Store\"):\n",
    "            f = os.path.join(directory, filename)\n",
    "            if os.path.isfile(f):\n",
    "                df = pd.read_csv(f, header=None)\n",
    "                data=df.drop(columns=187)\n",
    "                data=data.values\n",
    "                # S·ªë l∆∞·ª£ng l·∫∑p qua d·ªØ li·ªáu\n",
    "                num_samples = len(data) - window_input - window_out \n",
    "\n",
    "                if(num_samples>0):\n",
    "                    total_sample = total_sample + num_samples\n",
    "                    for i in range(num_samples):\n",
    "                        X_window = data[i:i+window_input]\n",
    "                        y_window = data[i+window_input+window_out:i+window_input+window_out+1]\n",
    "\n",
    "\n",
    "                        X.append(X_window)\n",
    "                        y.append(y_window)\n",
    "                else:\n",
    "                    missing_file.append(filename)\n",
    "    print(\"------üçí------\")\n",
    "    print(f\"Num of file in {data_set[istrainset]} set can not use due to its missing of length: {len(missing_file)}\")\n",
    "    print(f\"Number of sample: {len(y)}/{len(X)}/{total_sample}\")\n",
    "    print(f\"Missing files: {missing_file}\")\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------üçí------\n",
      "Num of file in train set can not use due to its missing of length: 0\n",
      "Number of sample: 69069/69069/69069\n",
      "Missing files: []\n",
      "------üçí------\n",
      "Num of file in test set can not use due to its missing of length: 0\n",
      "Number of sample: 19797/19797/19797\n",
      "Missing files: []\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_data(1)\n",
    "X_test, y_test = get_data(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train, y_train)\n",
    "test_dataset = Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Dataloader(train_dataset, 16,len(train_dataset))\n",
    "test_loader = Dataloader(test_dataset,16,len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 400, 187)\n",
      "(16, 187)\n"
     ]
    }
   ],
   "source": [
    "print(train_loader[0][0].shape)\n",
    "print(train_loader[0][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_reg(model_name,epochs):\n",
    "    # T·∫°o m·ªôt m√¥ h√¨nh LSTM\n",
    "    if model_name == \"LSTM\":\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(64,activation='relu' ,input_shape=(window_input, 187)))  # ƒê·∫∑t input_shape ph√π h·ª£p v·ªõi k√≠ch th∆∞·ªõc c·ªßa m·∫£ng X_train\n",
    "        model.add(Dense(187))# ƒê·∫∑t l·ªõp Dense ph√π h·ª£p v·ªõi k√≠ch th∆∞·ªõc c·ªßa m·∫£ng y_train\n",
    "\n",
    "        # Compile model\n",
    "        learning_rate = 0.01\n",
    "        adam = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)    \n",
    "        model.compile(loss='mean_squared_error',optimizer=adam, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4316/4316 [==============================] - 389s 90ms/step - loss: 0.0349 - accuracy: 0.1057 - val_loss: 0.0420 - val_accuracy: 0.0858\n",
      "Epoch 2/2\n",
      "4316/4316 [==============================] - 360s 83ms/step - loss: 0.0400 - accuracy: 0.1088 - val_loss: 0.0412 - val_accuracy: 0.0858\n",
      "Saved model to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model_name = model_name_reg_phase\n",
    "epochs = 2\n",
    "model = build_model_reg(model_name, epochs = epochs)\n",
    "model.fit(train_loader, validation_data=test_loader, verbose=1, epochs=epochs)\n",
    "model.save(f\"trained/{model_name}_Phase1_PastECG_FutureECG_FutureCls_{minute_input}-mininput_{minute_output}-minoutput.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(f\"trained/PastECG_FutureECG_FutureCls_{minute_input}-mininput_{minute_output}-minoutput.h5\")\n",
    "# # D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra\n",
    "# y_pred = model.predict(test_loader)\n",
    "# df = pd.DataFrame({\n",
    "#    'y_true': test_loader[0][1][0],\n",
    "#    'y_pred': y_pred[0]\n",
    "#    })\n",
    "# lines = df.plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_class(istrainset):\n",
    "    directory = f\"{data_path}{data_set[istrainset]}/\"\n",
    "    if istrainset == 1:\n",
    "        X, y = [], []\n",
    "        for filename in os.listdir(directory):\n",
    "            if (filename not in (missing_file_train)) and (filename not in (missing_file_test)) and (filename != \".DS_Store\"):\n",
    "                f = os.path.join(directory, filename)\n",
    "                if os.path.isfile(f):\n",
    "                    df = pd.read_csv(f, header=None)\n",
    "                    data=df.values\n",
    "\n",
    "                    X_window = data[:,:-1]\n",
    "                    y_window = data[:,-1]\n",
    "                    X.append(X_window)\n",
    "                    y.append(y_window)\n",
    "        print(f\"Number of sample: {len(y)}/{len(X)}\")\n",
    "        return X,y\n",
    "    else:\n",
    "        y = []\n",
    "        for filename in os.listdir(directory):\n",
    "            if (filename not in (missing_file_train)) and (filename not in (missing_file_test)) and (filename != \".DS_Store\"):\n",
    "                f = os.path.join(directory, filename)\n",
    "                if os.path.isfile(f):\n",
    "                    df = pd.read_csv(f, header=None)\n",
    "                    data=df.values\n",
    "\n",
    "                    y_window = data[window_input+window_out:,-1]\n",
    "                    y.append(y_window)\n",
    "        print(f\"Number of sample: {len(y)}\")\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sample: 55/55\n",
      "1237/1237 [==============================] - 47s 38ms/step\n",
      "Number of sample: 15\n"
     ]
    }
   ],
   "source": [
    "# M√¥ h√¨nh ph√¢n l·ªõp v·∫´n ph·∫£i ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n t·∫≠p train\n",
    "X_class_train, y_class_train = get_data_class(1)\n",
    "X_class_train= np.concatenate(X_class_train, axis=0)\n",
    "y_class_train= np.concatenate(y_class_train, axis=0)\n",
    "y_class_train=y_class_train.astype(int)\n",
    "\n",
    "#M√¥ h√¨nh ph√¢n l·ªõp d·ª± ƒëo√°n tr√™n ch√≠nh gi√° tr·ªã ƒë∆∞·ª£c pred t·ª´ m√¥ h√¨nh h·ªìi qui ·ªü pha tr∆∞·ªõc\n",
    "model = tf.keras.models.load_model(f\"trained/{model_name_reg_phase}_Phase1_PastECG_FutureECG_FutureCls_{minute_input}-mininput_{minute_output}-minoutput.h5\")\n",
    "# D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra\n",
    "y_pred = model.predict(test_loader)\n",
    "X_class_test = y_pred\n",
    "y_class_test = get_data_class(0)\n",
    "y_class_test=np.concatenate(y_class_test)\n",
    "y_class_test=y_class_test.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_cls(model_name,epochs):\n",
    "    # T·∫°o m·ªôt m√¥ h√¨nh LSTM\n",
    "    if model_name == \"LSTM\":\n",
    "        input_shape = (1, length_ecg)\n",
    "        model= Sequential()\n",
    "        model.add(LSTM(64, input_shape=input_shape, activation='relu', return_sequences=True))\n",
    "        model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "        model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "        model.add(LSTM(64, activation='relu'))\n",
    "        model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "        # Compile model\n",
    "        learning_rate = 0.01\n",
    "        adam = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)        \n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3534/3534 [==============================] - 8s 2ms/step - loss: 0.1749 - accuracy: 0.9567\n",
      "Epoch 2/10\n",
      "3534/3534 [==============================] - 6s 2ms/step - loss: 0.1318 - accuracy: 0.9670\n",
      "Epoch 3/10\n",
      "3534/3534 [==============================] - 6s 2ms/step - loss: 0.1258 - accuracy: 0.9695\n",
      "Epoch 4/10\n",
      "3534/3534 [==============================] - 6s 2ms/step - loss: 0.1129 - accuracy: 0.9720\n",
      "Epoch 5/10\n",
      "3534/3534 [==============================] - 6s 2ms/step - loss: 0.1184 - accuracy: 0.9707\n",
      "Epoch 6/10\n",
      "3534/3534 [==============================] - 6s 2ms/step - loss: 0.1210 - accuracy: 0.9721\n",
      "Epoch 7/10\n",
      "3534/3534 [==============================] - 6s 2ms/step - loss: 0.1405 - accuracy: 0.9657\n",
      "Epoch 8/10\n",
      "3534/3534 [==============================] - 6s 2ms/step - loss: 0.1392 - accuracy: 0.9671\n",
      "Epoch 9/10\n",
      "3534/3534 [==============================] - 6s 2ms/step - loss: 0.1294 - accuracy: 0.9692\n",
      "Epoch 10/10\n",
      "3534/3534 [==============================] - 6s 2ms/step - loss: 0.1256 - accuracy: 0.9693\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_name = model_name_reg_phase\n",
    "epochs = 10\n",
    "model = build_model_cls(model_name_cls_phase,epochs)\n",
    "model.fit(X_class_train.reshape(X_class_train.shape[0], 1, X_class_train.shape[1]), y_class_train, epochs=epochs, batch_size=32)\n",
    "model.save(f\"trained/{model_name}_Phase2_PastECG_FutureECG_FutureCls_{minute_input}-mininput_{minute_output}-minoutput.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (window_input, length_ecg)\n",
    "model= Sequential()\n",
    "model.add(LSTM(64, input_shape=input_shape, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "for layers in (model.layers)[:4]:\n",
    "    print(layers)\n",
    "    layers.trainable = False\n",
    "model.summary()\n",
    "model.layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
