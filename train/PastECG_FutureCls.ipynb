{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/MIT-BIH/'\n",
    "minute_input = 10\n",
    "minute_output = 10\n",
    "window_input= 40*minute_input\n",
    "window_out= 40*minute_input\n",
    "train_size = 0.8\n",
    "test_size = 1 - train_size\n",
    "# data_set = {\n",
    "#   0: \"test\",\n",
    "#   1: \"train\"\n",
    "# }\n",
    "\n",
    "data_set = {\n",
    "  0: \"test_ram\",\n",
    "  1: \"train_ram\"\n",
    "}\n",
    "\n",
    "# length_ecg l√† ƒë·ªô d√†i 2 kho·∫£ng RR ƒë∆∞·ª£c fixed l√∫c ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu \n",
    "# (ƒë·ªô d√†i m·ªôt d√≤ng trong file excel, tr·ª´ c·ªôt cu·ªëi l√† nh√£n l·ªõp b·ªánh tim)\n",
    "length_ecg = 187 \n",
    "batch_size = 16\n",
    "model_cls = \"LSTM\"\n",
    "'''\n",
    "ƒê·ªô d√†i c·ªßa input/output c√†ng d√†i th√¨ s·ªë l∆∞·ª£ng file kh√¥ng ƒë√°p ·ª©ng ƒë·ªß ƒë·ªÉ t·∫°o m·ªôt m·∫´u h·ª£p l·ªá \n",
    "cho m√¥ h√¨nh c√†ng nhi·ªÅu. ƒê·ªÉ ƒë√°m b·∫£o t√≠nh th·ªëng nh·∫•t n√™n s·∫Ω d√πng ƒë·ªô d√†i d√†i nh·∫•t c·ªßa ph·∫ßn \n",
    "input/output trong qu√° tr√¨nh th·ª±c nghi·ªám ƒë·ªÉ l√† chu·∫©n t·ª´ ƒë√≥ lo·∫°i c√°c file b·ªã thi·∫øu n√†y ƒë·ªÅu\n",
    "·ªü nh·ªØng ph·∫ßn th·ª±c nghi·ªám input/output kh√°c.\n",
    "'''\n",
    "missing_file_train = ['201_V1.csv', '102_V2.csv', '124_V4.csv', '112_V1.csv', '203_V1.csv', '116_V1.csv', '108_V1.csv', '207_V1.csv', '111_V1.csv', '200_V1.csv', '207_MLII.csv', '210_V1.csv', '202_V1.csv', '113_V1.csv', '214_V1.csv', '121_V1.csv', '109_V1.csv', '105_V1.csv', '107_V1.csv', '115_V1.csv', '208_V1.csv']\n",
    "missing_file_test = ['213_V1.csv', '231_V1.csv', '228_V1.csv', '222_V1.csv', '232_V1.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(istrainset):    \n",
    "    missing_file = []\n",
    "    total_sample = 0\n",
    "    directory = f\"{data_path}{data_set[istrainset]}/\"\n",
    "    X, y = [], []\n",
    "    for filename in os.listdir(directory):\n",
    "        if (filename not in (missing_file_train)) and (filename not in (missing_file_test)) and (filename != \".DS_Store\"):\n",
    "            f = os.path.join(directory, filename)\n",
    "            if os.path.isfile(f):\n",
    "                df = pd.read_csv(f, header=None)\n",
    "                data=df.drop(columns=length_ecg)\n",
    "                data=data.values\n",
    "                data1=df.iloc[:, length_ecg]\n",
    "                # S·ªë l∆∞·ª£ng l·∫∑p qua d·ªØ li·ªáu\n",
    "                num_samples = len(data) - window_input - 1 - window_out\n",
    "                # T·∫°o d·ªØ li·ªáu train t·ª´ c·ª≠a s·ªï tr∆∞·ª£t\n",
    "                if num_samples > 0:\n",
    "                    total_sample = total_sample + num_samples\n",
    "                    for i in range(num_samples):\n",
    "                        X_window = data[i:i+window_input]\n",
    "                        y_value = data1[i+window_input+window_out]\n",
    "\n",
    "                        X.append(X_window)\n",
    "                        y.append(y_value)\n",
    "                else:\n",
    "                    missing_file.append(filename)\n",
    "    print(\"------üçí------\")\n",
    "    print(f\"Num of file in {data_set[istrainset]} set can not use due to its missing of length: {len(missing_file)}\")\n",
    "    print(f\"Number of sample: {len(y)}/{len(X)}/{total_sample}\")\n",
    "    print(f\"Missing files: {missing_file}\")\n",
    "    return X,y\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # L·∫•y t·∫≠p train/test\n",
    "X_train, y_train = get_data(1)\n",
    "X_test, y_test = get_data(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # read data\n",
    "        X = self.data[i]\n",
    "        y = self.label[i].astype(int)\n",
    "        return X, y\n",
    "\n",
    "class Dataloader(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataset, batch_size,size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.size= size\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # collect batch data\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "\n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
    "        return tuple(batch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size //self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train, y_train)\n",
    "test_dataset = Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Dataloader(train_dataset, batch_size,len(train_dataset))\n",
    "test_loader = Dataloader(test_dataset,batch_size,len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_cls(model_name,epochs):\n",
    "    # T·∫°o m·ªôt m√¥ h√¨nh LSTM\n",
    "    if model_name == \"LSTM\":\n",
    "        input_shape = (window_input, length_ecg)\n",
    "        model= Sequential()\n",
    "        model.add(LSTM(64, input_shape=input_shape, activation='relu', return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(32, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "        # Compile model\n",
    "        learning_rate = 0.00000001\n",
    "        adam = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)        \n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    elif model_name == \"SVMSGDClassifier\":\n",
    "        model = SGDClassifier(learning_rate = 'constant', eta0 = 0.1, shuffle = False, max_iter = 1)\n",
    "    elif model_name == \"SVMRBF\":\n",
    "        model = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"LSTM\"\n",
    "epochs = 2\n",
    "model = build_model_cls(model_name, epochs = epochs)\n",
    "model.fit(train_loader, validation_data=test_loader, verbose=1, epochs=epochs)\n",
    "model.save(f\"trained/{model_name}_PastECG_FutureCls_{minute_input}-mininput_{minute_output}-minoutput.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMBRF for batch_size ram issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"SVMSGDClassifier\"\n",
    "epochs = 2\n",
    "model = build_model_cls(model_name, epochs = epochs)\n",
    "\n",
    "for k in range(epochs):\n",
    "    print(f\"-----Epoch {k+1}/{epochs}------\")\n",
    "    for i in tqdm(range(len(train_loader))):\n",
    "        X_train_batch = train_loader[i][0]\n",
    "        y_train_batch = train_loader[i][1]\n",
    "        \n",
    "\n",
    "        # Generate some sample data\n",
    "        X_train_batch = X_train_batch.reshape(batch_size, window_input*length_ecg)\n",
    "        \n",
    "        # Step 1: RBF Kernel Approximation\n",
    "        rbf_feature = RBFSampler(gamma=1, random_state=42)\n",
    "        X_train_rbf = rbf_feature.fit_transform(X_train_batch)\n",
    "        # X_test_rbf = rbf_feature.transform(X_test)\n",
    "\n",
    "        model.partial_fit(X_train_rbf,y_train_batch, classes=list(range(5)))\n",
    "\n",
    "pkl_filename = f\"trained/{model_name}_PastECG_FutureCls_{minute_input}-mininput_{minute_output}-minoutput.sav\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filename = f\"trained/{model_name}_PastECG_FutureCls_{minute_input}-mininput_{minute_output}-minoutput.sav\"\n",
    "loaded_model = pickle.load(open(pkl_filename, 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "y_pred_all = np.array([])\n",
    "for i in tqdm(range(len(test_loader))):\n",
    "    X_train_batch = train_loader[i][0]\n",
    "\n",
    "    X_test_batch = test_loader[i][0]\n",
    "    y_test_batch = test_loader[i][1]\n",
    "    \n",
    "\n",
    "    # Generate some sample data\n",
    "    X_train_batch = X_train_batch.reshape(batch_size, window_input*length_ecg)\n",
    "    X_test_batch = X_test_batch.reshape(batch_size, window_input*length_ecg)\n",
    "\n",
    "    \n",
    "    # Step 1: RBF Kernel Approximation\n",
    "    rbf_feature = RBFSampler(gamma=1, random_state=42)\n",
    "    rbf_feature.fit_transform(X_train_batch)\n",
    "    X_test_rbf = rbf_feature.transform(X_test_batch)\n",
    "\n",
    "    Y_preds = loaded_model.predict(X_test_batch)\n",
    "\n",
    "    y_pred_all = np.append(y_pred_all, Y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.shape)\n",
    "print(y_pred_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred_all)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMBRF for not care about ram issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_ml(istrainset):    \n",
    "    missing_file = []\n",
    "    total_sample = 0\n",
    "    directory = f\"{data_path}{data_set[istrainset]}/\"\n",
    "    X, y = [], []\n",
    "    for filename in os.listdir(directory):\n",
    "        if (filename not in (missing_file_train)) and (filename not in (missing_file_test)) and (filename != \".DS_Store\"):\n",
    "            f = os.path.join(directory, filename)\n",
    "            if os.path.isfile(f):\n",
    "                df = pd.read_csv(f, header=None)\n",
    "                data=df.drop(columns=length_ecg)\n",
    "                data=data.values\n",
    "                data1=df.iloc[:, length_ecg]\n",
    "                # S·ªë l∆∞·ª£ng l·∫∑p qua d·ªØ li·ªáu\n",
    "                num_samples = len(data) - window_input - 1 - window_out\n",
    "                # T·∫°o d·ªØ li·ªáu train t·ª´ c·ª≠a s·ªï tr∆∞·ª£t\n",
    "                if num_samples > 0:\n",
    "                    total_sample = total_sample + num_samples\n",
    "                    for i in range(num_samples):\n",
    "                        X_window = data[i:i+window_input]\n",
    "                        y_value = data1[i+window_input+window_out]\n",
    "\n",
    "                        X.append(X_window.reshape(window_input*length_ecg))\n",
    "                        y.append(y_value)\n",
    "                else:\n",
    "                    missing_file.append(filename)\n",
    "    print(\"------üçí------\")\n",
    "    print(f\"Num of file in {data_set[istrainset]} set can not use due to its missing of length: {len(missing_file)}\")\n",
    "    print(f\"Number of sample: {len(y)}/{len(X)}/{total_sample}\")\n",
    "    print(f\"Missing files: {missing_file}\")\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # L·∫•y t·∫≠p train/test\n",
    "X_train, y_train = get_data_for_ml(1)\n",
    "X_test, y_test = get_data_for_ml(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"SVMRBF\"\n",
    "epochs = 2\n",
    "model = build_model_cls(model_name, epochs = epochs)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pkl_filename = f\"trained/{model_name}_PastECG_FutureCls_{minute_input}-mininput_{minute_output}-minoutput.sav\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
