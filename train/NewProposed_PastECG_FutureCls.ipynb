{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D, BatchNormalization, Activation, Input, Dense, Flatten, Dropout, Concatenate\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/MIT-BIH/'\n",
    "minute_input = 10\n",
    "minute_output = 10\n",
    "window_input= 40*minute_input\n",
    "window_out= 40*minute_input\n",
    "train_size = 0.8\n",
    "test_size = 1 - train_size\n",
    "data_set = {\n",
    "  0: \"test\",\n",
    "  1: \"train\"\n",
    "}\n",
    "\n",
    "# data_set = {\n",
    "#   0: \"test_ram\",\n",
    "#   1: \"train_ram\"\n",
    "# }\n",
    "\n",
    "# length_ecg l√† ƒë·ªô d√†i 2 kho·∫£ng RR ƒë∆∞·ª£c fixed l√∫c ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu \n",
    "# (ƒë·ªô d√†i m·ªôt d√≤ng trong file excel, tr·ª´ c·ªôt cu·ªëi l√† nh√£n l·ªõp b·ªánh tim)\n",
    "length_ecg = 187 \n",
    "batch_size = 16\n",
    "model_cls = \"LSTM\"\n",
    "'''\n",
    "ƒê·ªô d√†i c·ªßa input/output c√†ng d√†i th√¨ s·ªë l∆∞·ª£ng file kh√¥ng ƒë√°p ·ª©ng ƒë·ªß ƒë·ªÉ t·∫°o m·ªôt m·∫´u h·ª£p l·ªá \n",
    "cho m√¥ h√¨nh c√†ng nhi·ªÅu. ƒê·ªÉ ƒë√°m b·∫£o t√≠nh th·ªëng nh·∫•t n√™n s·∫Ω d√πng ƒë·ªô d√†i d√†i nh·∫•t c·ªßa ph·∫ßn \n",
    "input/output trong qu√° tr√¨nh th·ª±c nghi·ªám ƒë·ªÉ l√† chu·∫©n t·ª´ ƒë√≥ lo·∫°i c√°c file b·ªã thi·∫øu n√†y ƒë·ªÅu\n",
    "·ªü nh·ªØng ph·∫ßn th·ª±c nghi·ªám input/output kh√°c.\n",
    "'''\n",
    "missing_file_train = ['201_V1.csv', '102_V2.csv', '124_V4.csv', '112_V1.csv', '203_V1.csv', '116_V1.csv', '108_V1.csv', '207_V1.csv', '111_V1.csv', '200_V1.csv', '207_MLII.csv', '210_V1.csv', '202_V1.csv', '113_V1.csv', '214_V1.csv', '121_V1.csv', '109_V1.csv', '105_V1.csv', '107_V1.csv', '115_V1.csv', '208_V1.csv']\n",
    "missing_file_test = ['213_V1.csv', '231_V1.csv', '228_V1.csv', '222_V1.csv', '232_V1.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(istrainset):    \n",
    "    missing_file = []\n",
    "    total_sample = 0\n",
    "    directory = f\"{data_path}{data_set[istrainset]}/\"\n",
    "    X, y = [], []\n",
    "    for filename in os.listdir(directory):\n",
    "        if (filename not in (missing_file_train)) and (filename not in (missing_file_test)) and (filename != \".DS_Store\"):\n",
    "            f = os.path.join(directory, filename)\n",
    "            if os.path.isfile(f):\n",
    "                df = pd.read_csv(f, header=None)\n",
    "                data=df.drop(columns=length_ecg)\n",
    "                data=data.values\n",
    "                data1=df.iloc[:, length_ecg]\n",
    "                # S·ªë l∆∞·ª£ng l·∫∑p qua d·ªØ li·ªáu\n",
    "                num_samples = len(data) - window_input - 1 - window_out\n",
    "                # T·∫°o d·ªØ li·ªáu train t·ª´ c·ª≠a s·ªï tr∆∞·ª£t\n",
    "                if num_samples > 0:\n",
    "                    total_sample = total_sample + num_samples\n",
    "                    for i in range(num_samples):\n",
    "                        X_window = data[i:i+window_input]\n",
    "                        y_value = data1[i+window_input+window_out]\n",
    "\n",
    "                        X.append(X_window)\n",
    "                        y.append(y_value)\n",
    "                else:\n",
    "                    missing_file.append(filename)\n",
    "    print(\"------üçí------\")\n",
    "    print(f\"Num of file in {data_set[istrainset]} set can not use due to its missing of length: {len(missing_file)}\")\n",
    "    print(f\"Number of sample: {len(y)}/{len(X)}/{total_sample}\")\n",
    "    print(f\"Missing files: {missing_file}\")\n",
    "    return X,y\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------üçí------\n",
      "Num of file in train set can not use due to its missing of length: 0\n",
      "Number of sample: 69014/69014/69014\n",
      "Missing files: []\n",
      "------üçí------\n",
      "Num of file in test set can not use due to its missing of length: 0\n",
      "Number of sample: 19782/19782/19782\n",
      "Missing files: []\n"
     ]
    }
   ],
   "source": [
    "# # L·∫•y t·∫≠p train/test\n",
    "X_train, y_train = get_data(1)\n",
    "X_test, y_test = get_data(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # read data\n",
    "        X = self.data[i]\n",
    "        y = self.label[i].astype(int)\n",
    "        return X, y\n",
    "\n",
    "class Dataloader(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataset, batch_size,size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.size= size\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # collect batch data\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "\n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
    "        return tuple(batch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size //self.batch_size\n",
    "    \n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "test_dataset = Dataset(X_test, y_test)\n",
    "train_loader = Dataloader(train_dataset, batch_size,len(train_dataset))\n",
    "test_loader = Dataloader(test_dataset,batch_size,len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 187)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train, y_train)\n",
    "test_dataset = Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Dataloader(train_dataset, batch_size,len(train_dataset))\n",
    "test_loader = Dataloader(test_dataset,batch_size,len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, kernel_size=3, dilation_rate=1):\n",
    "    # Convolutional block with batch normalization and ReLU activation\n",
    "    x_res = Conv1D(filters, kernel_size, dilation_rate=dilation_rate, padding='same')(x)\n",
    "    x_res = BatchNormalization()(x_res)\n",
    "    x_res = Activation('relu')(x_res)\n",
    "\n",
    "    # Residual connection\n",
    "    x_res = Conv1D(filters, kernel_size, dilation_rate=dilation_rate, padding='same')(x_res)\n",
    "    x_res = BatchNormalization()(x_res)\n",
    "    x_res = Activation('relu')(x_res)\n",
    "\n",
    "    x = tf.keras.layers.Add()([x, x_res])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_branch_extract_ecg():\n",
    "    inputs = Input(shape=(window_input, length_ecg))\n",
    "\n",
    "    # Convolutional block\n",
    "    x = Conv1D(64, kernel_size=3, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Residual blocks\n",
    "    for _ in range(3):  # You can adjust the number of residual blocks\n",
    "        x = residual_block(x, filters=64, kernel_size=3)\n",
    "\n",
    "    # Global average pooling\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_branch_extract_arrhythmia(model_name):\n",
    "    if model_name == \"CNN\":\n",
    "        inputs = Input(shape=(window_input, length_ecg))\n",
    "\n",
    "        # Convolutional block\n",
    "        x = Conv1D(64, kernel_size=3, padding='same')(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(3):  # You can adjust the number of residual blocks\n",
    "            x = residual_block(x, filters=64, kernel_size=3)\n",
    "\n",
    "        # Global average pooling\n",
    "        x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "    elif model_name == \"LSTM\":\n",
    "        input_layer = Input(shape=(window_input, length_ecg))\n",
    "        x = LSTM(64, activation='relu', return_sequences=True)(input_layer)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = LSTM(64, activation='relu', return_sequences=True)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = LSTM(64, activation='relu')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        x = Flatten()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_cls(model_name_arrbranch):\n",
    "    # First extract ECG branch\n",
    "    x_first_branch_input = Input(shape=(window_input, length_ecg))\n",
    "    # Convolutional block\n",
    "    x_first_branch = Conv1D(64, kernel_size=3, padding='same')(x_first_branch_input)\n",
    "    x_first_branch = BatchNormalization()(x_first_branch)\n",
    "    x_first_branch = Activation('relu')(x_first_branch)\n",
    "    # Residual blocks\n",
    "    for _ in range(3):  # You can adjust the number of residual blocks\n",
    "        x_first_branch = residual_block(x_first_branch, filters=64, kernel_size=3)\n",
    "    # Global average pooling\n",
    "    x_first_branch = tf.keras.layers.GlobalAveragePooling1D()(x_first_branch)\n",
    "    x_first_branch = Flatten()(x_first_branch)\n",
    "    x_first_branch = tf.keras.Model(inputs=x_first_branch_input, outputs=x_first_branch)\n",
    "\n",
    "    # Second extract Arrhythmia branch\n",
    "    if model_name_arrbranch == \"LSTM\":\n",
    "        x_second_branch_input = Input(shape=(window_input, length_ecg))\n",
    "        x_second_branch = LSTM(64, activation='relu', return_sequences=True)(x_second_branch_input)\n",
    "        x_second_branch = Dropout(0.2)(x_second_branch)\n",
    "        x_second_branch = LSTM(64, activation='relu', return_sequences=True)(x_second_branch)\n",
    "        x_second_branch = Dropout(0.2)(x_second_branch)\n",
    "        x_second_branch = LSTM(64, activation='relu')(x_second_branch)\n",
    "        x_second_branch = Dropout(0.2)(x_second_branch)\n",
    "        x_second_branch = Flatten()(x_second_branch)\n",
    "        x_second_branch = tf.keras.Model(inputs=x_second_branch_input, outputs=x_second_branch)\n",
    "    if model_name_arrbranch == \"CNN\":\n",
    "        x_second_branch_input = Input(shape=(window_input, length_ecg))\n",
    "\n",
    "        # Convolutional block\n",
    "        x_second_branch = Conv1D(64, kernel_size=3, padding='same')(x_second_branch_input)\n",
    "        x_second_branch = BatchNormalization()(x_second_branch)\n",
    "        x_second_branch = Activation('relu')(x_second_branch)\n",
    "        # Residual blocks\n",
    "        for _ in range(3):  # You can adjust the number of residual blocks\n",
    "            x_second_branch = residual_block(x_second_branch, filters=64, kernel_size=3)\n",
    "        # Global average pooling\n",
    "        x_second_branch = tf.keras.layers.GlobalAveragePooling1D()(x_second_branch)\n",
    "        x_second_branch = Flatten()(x_second_branch)\n",
    "        x_second_branch = tf.keras.Model(inputs=x_second_branch_input, outputs=x_second_branch)\n",
    "\n",
    "    concatenated = Concatenate()([x_first_branch.output, x_second_branch.output])\n",
    "    \n",
    "    x = Dense(64, activation='relu')(concatenated)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "\n",
    "\n",
    "    # Output layer for regression (linear activation)\n",
    "    outputs = Dense(5, activation='softmax')(x)\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    # Create and compile the model\n",
    "    model = tf.keras.Model(inputs=[x_first_branch.input,x_second_branch.input], outputs=outputs)\n",
    "    adam = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)    \n",
    "    model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedDataset:\n",
    "    def __init__(self, data1, data2, label):\n",
    "        self.data1 = data1\n",
    "        self.data2 = data2\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        X1 = self.data1[i]\n",
    "        X2 = self.data2[i]\n",
    "        y = self.label[i].astype(int)\n",
    "        return X1, X2, y\n",
    "\n",
    "class CombinedDataLoader(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataset, batch_size, size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.size = size\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "\n",
    "        data = [self.dataset[j] for j in range(start, stop)]\n",
    "\n",
    "        X1_batch = np.stack([sample[0] for sample in data], axis=0)\n",
    "        X2_batch = np.stack([sample[1] for sample in data], axis=0)\n",
    "        y_batch = np.array([sample[2] for sample in data])\n",
    "\n",
    "        return [X1_batch, X2_batch], y_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size // self.batch_size\n",
    "\n",
    "# Create the combined dataset\n",
    "combined_train_dataset = CombinedDataset(X_train, X_train, y_train)\n",
    "combined_train_loader = CombinedDataLoader(combined_train_dataset, batch_size, len(combined_train_dataset))\n",
    "\n",
    "# Create the combined dataset\n",
    "combined_test_dataset = CombinedDataset(X_test, X_test, y_test)\n",
    "combined_test_loader = CombinedDataLoader(combined_test_dataset, batch_size, len(combined_test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_arrbranch = \"CNN\"\n",
    "epochs = 2\n",
    "model = build_model_cls(model_name_arrbranch)\n",
    "# train_generator = combined_data_generator(train_loader, train_loader)\n",
    "# model.fit(train_generator, validation_data=test_loader, verbose=1, epochs=epochs)\n",
    "# model.save(f\"trained/New_Proposed_CNN_{model_name_arrbranch}_PastECG_FutureCls_{minute_input}-mininput_{minute_output}-minoutput.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  47/4313 [..............................] - ETA: 9:14 - loss: 0.7980 - accuracy: 0.8191ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x29e0d74f0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/keras/src/backend.py\", line 5159, in <genexpr>\n",
      "    ta.write(ta_index_to_write, out)  File \"/Users/trinhtruc/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "4313/4313 [==============================] - 572s 133ms/step - loss: 0.2517 - accuracy: 0.9448 - val_loss: 1.4492 - val_accuracy: 0.7904\n",
      "Epoch 2/2\n",
      "4313/4313 [==============================] - 567s 131ms/step - loss: 0.1856 - accuracy: 0.9571 - val_loss: 2.4998 - val_accuracy: 0.7270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2e7205e80>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "model.fit(combined_train_loader, validation_data=combined_test_loader,verbose=1, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"LSTM\"\n",
    "epochs = 2\n",
    "model = build_model_cls(model_name, epochs = epochs)\n",
    "model.fit(train_loader, validation_data=test_loader, verbose=1, epochs=epochs)\n",
    "\n",
    "model.fit([train_loader, train_loader], epochs=10, batch_size=32)\n",
    "\n",
    "model.save(f\"trained/{model_name}_PastECG_FutureCls_{minute_input}-mininput_{minute_output}-minoutput.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30582, 7480)\n",
      "(30582,)\n"
     ]
    }
   ],
   "source": [
    "x = np.load(\"X_test_1_1.npy\")\n",
    "y = np.load(\"y_test_1_1.npy\")\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"/Users/trinhtruc/X_test_10_1.npy\")\n",
    "# y = np.load(\"y_train_1_1.npy\")\n",
    "print(x.shape)\n",
    "# print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
